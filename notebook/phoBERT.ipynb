{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_dataset.csv')\n",
    "df_val = pd.read_csv('../data/valid_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(df):\n",
    "    df['label'] = 0\n",
    "    df.loc[df['genre'] == \"giao-duc\", \"label\"] = 1\n",
    "    df.loc[df['genre'] == \"xe\", \"label\"] = 2\n",
    "    df.loc[df['genre'] == \"suc-khoe\", \"label\"] = 3\n",
    "    df.loc[df['genre'] == \"cong-nghe-game\", \"label\"] = 4\n",
    "\n",
    "numerize(df)\n",
    "numerize(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Dataset.from_pandas(df)\n",
    "df_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "dataset = {\n",
    "    'train': df_train,\n",
    "    'validation': df_val,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vinai/phobert-base\n",
    "model_name = \"vinai/phobert-base\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    tokenized_inputs = tokenizer(examples['title'], padding=True, truncation=True )\n",
    "    return tokenized_inputs\n",
    "\n",
    "dataset['train'] = dataset['train'].map(tokenize, batched=True, batch_size=None)\n",
    "dataset['validation'] = dataset['validation'].map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(df[\"label\"]))\n",
    "id2label = {k:v for k,v in enumerate(labels)}\n",
    "label2id = {v:k for k,v in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "config = (AutoConfig\n",
    "          .from_pretrained(model_name, num_labels=num_labels,\n",
    "                           label2id=label2id, id2label=id2label))\n",
    "\n",
    "# model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 186/630 [01:46<03:50,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1786, 'grad_norm': 10.731210708618164, 'learning_rate': 1.4095238095238097e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|███       | 189/630 [01:48<03:38,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25506022572517395, 'eval_accuracy': 0.92, 'eval_f1': 0.9225972273340695, 'eval_runtime': 0.5718, 'eval_samples_per_second': 174.871, 'eval_steps_per_second': 12.241, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 248/630 [02:18<03:09,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0987, 'grad_norm': 0.2759944796562195, 'learning_rate': 1.2126984126984127e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|████      | 252/630 [02:20<02:54,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30083468556404114, 'eval_accuracy': 0.93, 'eval_f1': 0.9315246753823344, 'eval_runtime': 0.5646, 'eval_samples_per_second': 177.113, 'eval_steps_per_second': 12.398, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 310/630 [02:50<02:39,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.06, 'grad_norm': 0.5198515057563782, 'learning_rate': 1.015873015873016e-05, 'epoch': 4.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 315/630 [02:53<02:25,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3532783091068268, 'eval_accuracy': 0.93, 'eval_f1': 0.9306171314231739, 'eval_runtime': 0.5603, 'eval_samples_per_second': 178.473, 'eval_steps_per_second': 12.493, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 372/630 [03:22<02:15,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0329, 'grad_norm': 17.85242462158203, 'learning_rate': 8.190476190476192e-06, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 378/630 [03:25<01:56,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32787489891052246, 'eval_accuracy': 0.94, 'eval_f1': 0.9409094970070581, 'eval_runtime': 0.5754, 'eval_samples_per_second': 173.782, 'eval_steps_per_second': 12.165, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 434/630 [03:53<01:37,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0424, 'grad_norm': 0.09025076776742935, 'learning_rate': 6.222222222222223e-06, 'epoch': 6.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 441/630 [03:57<01:26,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.373673677444458, 'eval_accuracy': 0.93, 'eval_f1': 0.9320572450805009, 'eval_runtime': 0.5606, 'eval_samples_per_second': 178.367, 'eval_steps_per_second': 12.486, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 496/630 [04:24<01:06,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0198, 'grad_norm': 0.11602314561605453, 'learning_rate': 4.2539682539682546e-06, 'epoch': 7.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|████████  | 504/630 [04:31<01:33,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35336875915527344, 'eval_accuracy': 0.94, 'eval_f1': 0.9409094970070581, 'eval_runtime': 0.6038, 'eval_samples_per_second': 165.605, 'eval_steps_per_second': 11.592, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 558/630 [04:59<00:36,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0174, 'grad_norm': 0.06966102868318558, 'learning_rate': 2.285714285714286e-06, 'epoch': 8.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|█████████ | 567/630 [05:04<00:29,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3715101182460785, 'eval_accuracy': 0.93, 'eval_f1': 0.9320572450805009, 'eval_runtime': 0.6003, 'eval_samples_per_second': 166.587, 'eval_steps_per_second': 11.661, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 620/630 [05:31<00:05,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0098, 'grad_norm': 0.07069993764162064, 'learning_rate': 3.174603174603175e-07, 'epoch': 9.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 630/630 [05:36<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.374647855758667, 'eval_accuracy': 0.93, 'eval_f1': 0.9320572450805009, 'eval_runtime': 0.588, 'eval_samples_per_second': 170.074, 'eval_steps_per_second': 11.905, 'epoch': 10.0}\n",
      "{'train_runtime': 336.7096, 'train_samples_per_second': 29.699, 'train_steps_per_second': 1.871, 'train_loss': 0.2042935320782283, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.2042935320782283, metrics={'train_runtime': 336.7096, 'train_samples_per_second': 29.699, 'train_steps_per_second': 1.871, 'train_loss': 0.2042935320782283, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "                    output_dir=\"./results\",\n",
    "                    num_train_epochs=10,\n",
    "                    learning_rate=2e-5,\n",
    "                    per_device_train_batch_size=16,\n",
    "                    per_device_eval_batch_size=16,\n",
    "                    weight_decay=0.01,\n",
    "                    logging_steps=len(dataset[\"train\"]) // 16,\n",
    "                    evaluation_strategy='epoch',\n",
    "                    eval_steps=10,\n",
    "                    report_to='none'  \n",
    "            )\n",
    "\n",
    "trainer = Trainer(\n",
    "                  model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=dataset[\"train\"],\n",
    "                  eval_dataset=dataset[\"validation\"],\n",
    "                  data_collator=data_collator,\n",
    "                  tokenizer=tokenizer\n",
    "            )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mob2711/phoBERT_finetune_news_classification\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mob2711/phoBERT_finetune_news_classification\")\n",
    "\n",
    "def tokenize(text):\n",
    "    encoded_text = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return encoded_text\n",
    "\n",
    "test = pd.read_csv('../data/test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text_data):\n",
    "    encoded_data = tokenize(text_data)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_data)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "def predict_prob(text_data):\n",
    "    encoded_data = tokenize(text_data)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_data)\n",
    "        logits = outputs.logits\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions.cpu().numpy(), probabilities.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test)\n",
    "\n",
    "f1 = f1_score(test['label'], predictions, average='weighted')\n",
    "\n",
    "recall = recall_score(test['label'], predictions, average='weighted')\n",
    "\n",
    "precision = precision_score(test['label'], predictions, average='weighted')\n",
    "\n",
    "accuracy = accuracy_score(test['label'], predictions)\n",
    "\n",
    "print('accuracy: ' + str(accuracy))\n",
    "print('recall: ' + str(recall))\n",
    "print('precision: ' + str(precision))\n",
    "print('f1: ' + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
